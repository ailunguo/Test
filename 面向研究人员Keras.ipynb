{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCaTV5VVaiRDgfsq5l+h5G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ailunguo/Test/blob/main/%E9%9D%A2%E5%90%91%E7%A0%94%E7%A9%B6%E4%BA%BA%E5%91%98Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ipNaA6bvRc0l"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 张量"
      ],
      "metadata": {
        "id": "D_o3GclkSfun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 常数张量\n",
        "x = tf.constant([[5, 2], [1, 3]])\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYAG6DbFRnJp",
        "outputId": "ba25ccc1-e82f-448c-bcbb-cb706b8501b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
              "array([[5, 2],\n",
              "       [1, 3]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eTbD-1wRw9o",
        "outputId": "a7339fa0-1d03-4b0f-a606-14fd4a587001"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 2],\n",
              "       [1, 3]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"dtype:\", x.dtype)\n",
        "print(\"shape:\", x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09e47VvQR0Nw",
        "outputId": "0de69b87-bd0e-4e5b-b0ec-54dec6dffe6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dtype: <dtype: 'int32'>\n",
            "shape: (2, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.ones(shape=(2, 1)))\n",
        "print(tf.zeros(shape=(2, 1)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l46zoqjR99Q",
        "outputId": "fb4bd601-6c3d-4461-faca-60c72d945572"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.]\n",
            " [0.]], shape=(2, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建随机常数张量\n",
        "x = tf.random.normal(shape=(2, 2), mean=0.0, stddev=1.0)\n",
        "\n",
        "x = tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')"
      ],
      "metadata": {
        "id": "dQfuFig9SGlC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 变量"
      ],
      "metadata": {
        "id": "najKaRc6SjRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 变量是用于存储可变状态(例如神经网络的权重)的特殊张量。\n",
        "initial_value = tf.random.normal(shape=(2, 2))\n",
        "a = tf.Variable(initial_value)\n",
        "print(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcuW_B-aScn_",
        "outputId": "0974931d-402b-4d33-bac6-b26308de71fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[ 0.8556467 , -0.9760387 ],\n",
            "       [ 0.11634047,  0.917724  ]], dtype=float32)>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Variable可以使用.assign(value),.assign_add(increment)来更新值.assign_sub(decrement)\n",
        "new_value = tf.random.normal(shape=(2, 2))\n",
        "a.assign(new_value)\n",
        "for i in range(2):\n",
        "  for j in range(2):\n",
        "    assert a[i, j] == new_value[i, j]\n",
        "\n",
        "added_value = tf.random.normal(shape=(2, 2))\n",
        "a.assign_add(added_value)\n",
        "for i in range(2):\n",
        "  for j in range(2):\n",
        "    assert a[i, j] == new_value[i, j] + added_value[i, j]"
      ],
      "metadata": {
        "id": "pRHJHEzgTCex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 在Tensorflow中的数学"
      ],
      "metadata": {
        "id": "g1_QP7ZLV_sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.random.normal(shape=(2, 2))\n",
        "b = tf.random.normal(shape=(2, 2))\n",
        "\n",
        "c = a + b\n",
        "d = tf.square(c)\n",
        "e = tf.exp(d)"
      ],
      "metadata": {
        "id": "c42qzFgFUrSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 可微\n",
        "a = tf.random.normal(shape=(2, 2))\n",
        "b = tf.random.normal(shape=(2, 2))\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(a)\n",
        "  c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "  dc_da = tape.gradient(c, a)\n",
        "  print(dc_da)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBE8G8pxU12R",
        "outputId": "fa2a8e27-f12b-43ef-d49b-0d29f71a5f47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.6820381   0.9994569 ]\n",
            " [-0.67598087 -0.59693843]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.Variable(a)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "  dc_da = tape.gradient(c, a)\n",
        "  print(dc_da)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6f8kWRBU4jQ",
        "outputId": "70e388a6-b232-432c-e235-ea291add7a5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.6820381   0.9994569 ]\n",
            " [-0.67598087 -0.59693843]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过嵌套来计算高阶导数\n",
        "with tf.GradientTape() as outer_tape:\n",
        "  with tf.GradientTape() as tape:\n",
        "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "    dc_da = tape.gradient(c, a)\n",
        "  d2c_d2a = outer_tape.gradient(dc_da, a)\n",
        "  print(d2c_d2a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Htm-MnpU6Mo",
        "outputId": "9279c049-537a-4548-848d-67fbcba53126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0.3676372  0.00111532]\n",
            " [1.05185    0.55841327]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keras层"
      ],
      "metadata": {
        "id": "E3VjIh8zljBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32, input_dim=32):\n",
        "    super().__init__()\n",
        "    self.w = self.add_weight(\n",
        "        shape=(input_dim, units),\n",
        "        initializer='random_normal',\n",
        "        trainable=True)\n",
        "    self.b = self.add_weight(\n",
        "        shape=(units,),\n",
        "        initializer='zeros',\n",
        "        trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b"
      ],
      "metadata": {
        "id": "3VYrXZ3ClgRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_layer = Linear(units=4, input_dim=2)\n",
        "\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert y.shape == (2, 4)"
      ],
      "metadata": {
        "id": "NhncA0yEnhtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "metadata": {
        "id": "_gBftVBjnneA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 层权重创建build(input_shape)"
      ],
      "metadata": {
        "id": "QvI-99w_ph1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear(keras.layers.Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32):\n",
        "    super().__init__()\n",
        "    self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.w = self.add_weight(\n",
        "        shape=(input_shape[-1], self.units),\n",
        "        initializer='random_normal',\n",
        "        trainable=True\n",
        "    )\n",
        "    self.b = self.add_weight(\n",
        "        shape=(self.units,),\n",
        "        initializer='random_normal',\n",
        "        trainable=True\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "linear_layer = Linear(4)\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "# 上述类Linear中的build()函数是在执行call()时才执行的"
      ],
      "metadata": {
        "id": "NcxOZTTlpc9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 网络层的梯度"
      ],
      "metadata": {
        "id": "3qj7VxWbssPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare a dataset\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "linear_layer = Linear(10)\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = linear_layer(x)\n",
        "\n",
        "    loss = loss_fn(y, logits)\n",
        "\n",
        "  gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print('Step:',step, 'loss:', float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xd9tA9SpsDLh",
        "outputId": "00a87739-6590-401f-ddd1-0c4a885affb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n",
            "Step: 0 loss: 2.345146656036377\n",
            "Step: 100 loss: 2.2982373237609863\n",
            "Step: 200 loss: 2.1140570640563965\n",
            "Step: 300 loss: 2.0613176822662354\n",
            "Step: 400 loss: 1.939256191253662\n",
            "Step: 500 loss: 1.8842382431030273\n",
            "Step: 600 loss: 1.7714017629623413\n",
            "Step: 700 loss: 1.7904040813446045\n",
            "Step: 800 loss: 1.6409804821014404\n",
            "Step: 900 loss: 1.6009747982025146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 可训练和不可训练的权重"
      ],
      "metadata": {
        "id": "7s_VBcPbwxBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过trainable_weights和non_trainable_weights来设置可训练和不可训练的权重\n",
        "class ComputeSum(keras.layers.Layer):\n",
        "  \"\"\"Returns the sum of the inputs.\"\"\"\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "    super().__init__()\n",
        "    self.total = self.add_weight(\n",
        "        initializer='zeros',\n",
        "        shape=(input_dim,),\n",
        "        trainable=False\n",
        "    )\n",
        "\n",
        "  def call(self, inputs):\n",
        "    self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "    return self.total\n",
        "\n",
        "my_sum = ComputeSum(2)\n",
        "x = tf.ones((2, 2))\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UmsvNjxsEUD",
        "outputId": "a9f57602-f587-4418-f259-8aab51c9dce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 嵌套层"
      ],
      "metadata": {
        "id": "ilGnh8oVAJCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 多次利用Linear这个类\n",
        "\n",
        "class MLP(keras.layers.Layer):\n",
        "  \"\"\"简单的堆叠层\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear_1 = Linear(32)\n",
        "    self.linear_2 = Linear(32)\n",
        "    self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.linear_1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.linear_2(x)\n",
        "    x = tf.nn.relu(x)\n",
        "    return self.linear_3(x)\n",
        "\n",
        "mlp = MLP()\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "assert len(mlp.weights) == 6"
      ],
      "metadata": {
        "id": "VV65-DFPxxR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(mlp.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nc5DYymyBZTe",
        "outputId": "7c239c1d-adff-4afa-fc79-0d4cc14ce260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 上面的MLP类，相当于下面的\n",
        "mlp = keras.Sequential(\n",
        "    [keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "     keras.layers.Dense(32, activation=tf.nn.relu),\n",
        "     keras.layers.Dense(10),]\n",
        ")"
      ],
      "metadata": {
        "id": "XkcXVbUxBbNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# len(mlp.weights) # 创建完直接看权重的个数是看不到的，因为build()还没有执行\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "len(mlp.weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7peO6RsRB3IV",
        "outputId": "cf4c07e5-e732-4390-9739-86e1254bcbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 追踪各层造成的损失"
      ],
      "metadata": {
        "id": "AoAD5HSRCtNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建正则化损失的层\n",
        "class ActivityRegularization(keras.layers.Layer):\n",
        "  \"\"\"Layer that creates an activity sparsity regularization loss\"\"\"\n",
        "\n",
        "  def __init__(self, rate=1e-2):\n",
        "    super().__init__()\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs):\n",
        "    self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "AKKSjNueB48z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 包含该层的任何模型都将跟踪此正则化损失\n",
        "class SparseMLP(keras.layers.Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear_1 = Linear(32)\n",
        "    self.regularization = ActivityRegularization(1e-2)\n",
        "    self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.linear_1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.regularization(x)\n",
        "    return self.linear_3(x)\n",
        "\n",
        "mlp = SparseMLP()\n",
        "y = mlp(tf.ones((10, 10)))\n",
        "print(mlp.losses)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KTiLByJDiD0",
        "outputId": "481e3a8c-2df9-41e5-eaaf-3833d86bfef6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<tf.Tensor: shape=(), dtype=float32, numpy=0.20146622>]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 这些损失会在每次向前传播开始时由顶层清除，它们不会积累。layer.losser始终仅包含最后一次前向传播期间的损失\n",
        "\n",
        "mlp = SparseMLP()\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1\n",
        "\n",
        "# 接下来，用这些损失在训练中\n",
        "\n",
        "(x_train, y_train), _ = keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "mlp = SparseMLP()\n",
        "\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = mlp(x)\n",
        "    loss = loss_fn(y, logits)\n",
        "    loss += sum(mlp.losses) # 将正则化损失项加入到损失中\n",
        "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "\n",
        "  if step % 100 == 0:\n",
        "    print(\"step:\", step, \"Loss:\", float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBE0bFY7EkRF",
        "outputId": "d65c0735-968a-4677-c9c0-602f722a8884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step: 0 Loss: 5.145801067352295\n",
            "step: 100 Loss: 2.561513900756836\n",
            "step: 200 Loss: 2.4049787521362305\n",
            "step: 300 Loss: 2.3597991466522217\n",
            "step: 400 Loss: 2.3327791690826416\n",
            "step: 500 Loss: 2.3470520973205566\n",
            "step: 600 Loss: 2.3159615993499756\n",
            "step: 700 Loss: 2.33622407913208\n",
            "step: 800 Loss: 2.3366894721984863\n",
            "step: 900 Loss: 2.334726572036743\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 跟踪训练指标"
      ],
      "metadata": {
        "id": "Jz1A7WpIIqb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 在keras中提供了广泛的内置指标,keras.metrics.AUC, keras.metrics.PrecisionAtRecall等\n",
        "# 下面是一个简单的例子\n",
        "\n",
        "accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(32, activation='relu'),\n",
        "        keras.layers.Dense(32, activation='relu'),\n",
        "        keras.layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "for epoch in range(2):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      logits = model(x)\n",
        "      loss_value = loss_fn(y, logits)\n",
        "    accuracy.update_state(y, logits)\n",
        "\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "    if step % 200 == 0:\n",
        "      print(\"Epoch:\", epoch, \"Step:\", step)\n",
        "      print(\"Total running accuracy so far: %.3f\" % accuracy.result())\n",
        "\n",
        "  accuracy.reset_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hju7knZSIC3M",
        "outputId": "e1d34f83-73a3-4ce7-e99f-2e2d7b0f4e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Step: 0\n",
            "Total running accuracy so far: 0.094\n",
            "Epoch: 0 Step: 200\n",
            "Total running accuracy so far: 0.755\n",
            "Epoch: 0 Step: 400\n",
            "Total running accuracy so far: 0.830\n",
            "Epoch: 0 Step: 600\n",
            "Total running accuracy so far: 0.861\n",
            "Epoch: 0 Step: 800\n",
            "Total running accuracy so far: 0.878\n",
            "Epoch: 1 Step: 0\n",
            "Total running accuracy so far: 0.953\n",
            "Epoch: 1 Step: 200\n",
            "Total running accuracy so far: 0.943\n",
            "Epoch: 1 Step: 400\n",
            "Total running accuracy so far: 0.945\n",
            "Epoch: 1 Step: 600\n",
            "Total running accuracy so far: 0.945\n",
            "Epoch: 1 Step: 800\n",
            "Total running accuracy so far: 0.945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 还可以通过子类来定义自己的指标keras.metrics.Metric\n",
        "# 实现F1分数指标,支持样本加权\n",
        "class F1Score(keras.metrics.Metric):\n",
        "  def __init__(self, name='f1_score', dtype='float32', threshold=0.5, **kwargs):\n",
        "    super().__init__(name=name, dtype=dtype, **kwargs)\n",
        "    self.threshold = 0.5\n",
        "    self.true_positives = self.add_weight(\n",
        "        name='tp', dtype=dtype, initializer='zeros'\n",
        "    )\n",
        "    self.false_positives = self.add_weight(\n",
        "        name='fp', dtype=dtype, initializer='zeros'\n",
        "    )\n",
        "    self.false_negatives = self.add_weight(\n",
        "        name='fn', dtype=dtype, initializer='zeros'\n",
        "    )\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_pred = tf.math.greater_equal(y_pred, self.threshold)\n",
        "    y_true = tf.cast(y_true, tf.bool)\n",
        "    y_pred = tf.cast(y_pred, tf.bool)\n",
        "\n",
        "    true_positives = tf.cast(y_true & y_pred, self.dtype)\n",
        "    false_positives = tf.cast(~y_true & y_pred, self.dtype)\n",
        "    false_negatives = tf.cast(y_true & ~y_pred, self.dtype)\n",
        "\n",
        "    if sample_weight is not None:\n",
        "      sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "      true_positives *= sample_weight\n",
        "      false_positives *= sample_weight\n",
        "      false_negatives *= sample_weight\n",
        "\n",
        "    self.true_positives.assign_add(tf.reduce_sum(true_positives))\n",
        "    self.false_positives.assign_add(tf.reduce_sum(false_positives))\n",
        "    self.false_negatives.assign_add(tf.reduce_sum(false_negatives))\n",
        "\n",
        "  def result(self):\n",
        "    precision = self.true_positives / (self.true_positives + self.false_positives)\n",
        "    recall = self.true_positives / (self.true_positives + self.false_negatives)\n",
        "    return precision * recall * 2.0 / (precision + recall)\n",
        "\n",
        "  def reset_state(self):\n",
        "    self.true_positives.assign(0)\n",
        "    self.false_positives.assign(0)\n",
        "    self.false_negatives.assign(0)"
      ],
      "metadata": {
        "id": "IyS69DL3OxZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = F1Score()\n",
        "m.update_state([0, 1, 0, 0], [0.3, 0.5, 0.8, 0.9])\n",
        "print(\"Intermediate result:\", float(m.result()))\n",
        "\n",
        "m.update_state([1,1,1,1], [0.1,0.7,0.6,0.0])\n",
        "print(\"Final result:\", float(m.result()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhxikIihULCX",
        "outputId": "2ff0f87e-b793-4a8b-b300-989c3a34a39c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intermediate result: 1.0\n",
            "Final result: 0.6000000238418579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = tf.cast([1,1,0,0], tf.bool)\n",
        "true = tf.cast([1,0,0,0], tf.bool)\n",
        "tf.cast(pred & true, 'float32')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkxbnzdtVcR2",
        "outputId": "7486f81a-050a-4880-cf08-155228f4a022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(4,), dtype=float32, numpy=array([1., 0., 0., 0.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_sum([1,1,0,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0X_1gQ6ZuJO",
        "outputId": "0d0bf2b4-0d96-45ac-a6fd-fa13d4a45167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 编译函数"
      ],
      "metadata": {
        "id": "nHyuUsqMcMGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 急切运行对调试来说非常有用，但是通过将计算编译成静态图，将获得更好的性能\n",
        "# 静态图是研究人员最好的朋友，可以通过将任何函数包装在tf.function装饰器中来编译它\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.Dense(32, activation='relu'),\n",
        "        keras.layers.Dense(32, activation='relu'),\n",
        "        keras.layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "@tf.function # Make it fast\n",
        "def train_on_batch(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    logits = model(x)\n",
        "    loss = loss_fn(y, logits)\n",
        "    gradients = tape.gradient(loss, model.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "  return loss\n",
        "\n",
        "# Prepare a dataset\n",
        "(x_train, y_train),_ = keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train)\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  loss = train_on_batch(x, y) # a fast process\n",
        "  if step % 100 == 0:\n",
        "    print(\"Step:\", step, \"Loss:\", float(loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZHFC4p6cDh-",
        "outputId": "39617e57-cea6-4c9f-b282-2226850a5a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0 Loss: 2.294524908065796\n",
            "Step: 100 Loss: 0.5501132011413574\n",
            "Step: 200 Loss: 0.5192018747329712\n",
            "Step: 300 Loss: 0.3526468575000763\n",
            "Step: 400 Loss: 0.1700807809829712\n",
            "Step: 500 Loss: 0.29131579399108887\n",
            "Step: 600 Loss: 0.37472790479660034\n",
            "Step: 700 Loss: 0.36151182651519775\n",
            "Step: 800 Loss: 0.2379852533340454\n",
            "Step: 900 Loss: 0.18078218400478363\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 训练模式和推理模式"
      ],
      "metadata": {
        "id": "duVSrfbRfv7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 有些层，特别是BatchNormalization层和Dropout层,在训练和推理过程中具有不同的行为\n",
        "# 对于此类层，标准做法是在方法中公开training(布尔)参数call\n",
        "# 通过公开参数call，可以启用内置训练和评估循环(例如拟合)以在训练和推理模式中正确使用该层\n",
        "\n",
        "class Dropout(keras.layers.Layer):\n",
        "  def __init__(self, rate):\n",
        "    super().__init__()\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "      return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "\n",
        "class MLPWithDropout(keras.layers.Layer):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear_1 = Linear(32)\n",
        "    self.dropout = Dropout(0.5)\n",
        "    self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    x = self.linear_1(inputs)\n",
        "    x = tf.nn.relu(x)\n",
        "    x = self.dropout(x, training=training)\n",
        "    return self.linear_3(x)\n",
        "\n",
        "mlp = MLPWithDropout()\n",
        "y_train = mlp(tf.ones((2, 2)), training=True)\n",
        "y_test = mlp(tf.ones((2, 2)), training=False)"
      ],
      "metadata": {
        "id": "zPwoomCRcPUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 用于模型构建的函数式API"
      ],
      "metadata": {
        "id": "l9hRTo4pkSA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 要构建深度学习模型，，您不必一直使用面向对象编程，目前为止我们看到的所有层也可以按功能组合\n",
        "\n",
        "# We use an \"Input\" object to describe the shape and dtype of the inputs\n",
        "# This is the deep learning equivalent of *declaring a type*\n",
        "# The shape argument is per-sample; it does not include the batch size\n",
        "# The functional API focused on defining per-sample transformations\n",
        "# The model we create will automatically batch the per-sample transformations\n",
        "# so that it can be called on batched of data\n",
        "inputs = keras.Input(shape=(16,), dtype='float32')\n",
        "\n",
        "# We call layers on these 'type' objects\n",
        "# and they return updated types (new shapes/dtypes)\n",
        "x = Linear(32)(inputs)\n",
        "x = Dropout(0.5)(x)\n",
        "outputs = Linear(10)(x)\n",
        "\n",
        "# A functional 'Model' can be defined by specifying inputs and outputs\n",
        "# A model is itself a layer like any other\n",
        "model = keras.Model(inputs, outputs)\n",
        "assert len(model.weights) == 4\n",
        "\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)\n",
        "\n",
        "y = model(tf.ones((2, 16)), training=True)"
      ],
      "metadata": {
        "id": "fWg6akDIfylV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 内置的训练和评估循环"
      ],
      "metadata": {
        "id": "MPfs6HChotxO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(784,), dtype='float32')\n",
        "x = keras.layers.Dense(32, activation='relu')(inputs)\n",
        "x = keras.layers.Dense(32, activation='relu')(x)\n",
        "outputs = keras.layers.Dense(10)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "# Specify the loss, optimizer, and metrics with 'compile()'\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics = [keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train the model with the dataset for 2 epochs\n",
        "model.fit(dataset, epochs=2)\n",
        "model.predict(dataset)\n",
        "model.evaluate(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRyZAXudiCZt",
        "outputId": "e9a8aed1-98d3-4432-a95c-010935aa5ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.4105 - sparse_categorical_accuracy: 0.8795\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 2s 2ms/step - loss: 0.1924 - sparse_categorical_accuracy: 0.9447\n",
            "938/938 [==============================] - 1s 1ms/step\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.1586 - sparse_categorical_accuracy: 0.9526\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.1586298793554306, 0.9526166915893555]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 如果想利用面向对象模型的内置训练循环，可以对该Model类进行子类化\n",
        "# Layer只需重写即可\n",
        "class CustomModel(keras.Model):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "    self.loss_tracker = keras.metrics.Mean(name='loss')\n",
        "    self.accuracy = keras.metrics.SparseCategoricalAccuracy()\n",
        "    self.loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    self.optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "  def train_step(self, data):\n",
        "    x, y = data\n",
        "    with tf.GradientTape() as tape:\n",
        "      y_pred = self(x, training=True)\n",
        "      loss = self.loss_fn(y, y_pred)\n",
        "    gradients = tape.gradient(loss, self.trainable_weights)\n",
        "    self.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\n",
        "    self.loss_tracker.update_state(loss)\n",
        "    self.accuracy.update_state(y, y_pred)\n",
        "    return {'loss': self.loss_tracker.result(), 'accuracy': self.accuracy.result()}\n",
        "\n",
        "  @property\n",
        "  def metrics(self):\n",
        "    return [self.loss_tracker, self.accuracy]\n",
        "\n",
        "inputs = keras.Input(shape=(784,), dtype='float32')\n",
        "x = keras.layers.Dense(32, activation='relu')(inputs)\n",
        "x = keras.layers.Dense(32, activation='relu')(x)\n",
        "outputs = keras.layers.Dense(10)(x)\n",
        "model = CustomModel(inputs, outputs)\n",
        "model.compile()\n",
        "model.fit(dataset, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iXZ4gzockLc2",
        "outputId": "e334cbc5-3e84-4252-e7c9-9f68487d79d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "938/938 [==============================] - 2s 1ms/step - loss: 0.4034 - accuracy: 0.7946\n",
            "Epoch 2/2\n",
            "938/938 [==============================] - 1s 1ms/step - loss: 0.2035 - accuracy: 0.9361\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bbcfb06d7e0>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 通过@property装饰器使metrics为一个属性\n",
        "model.metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3aWFxE0uH1W",
        "outputId": "701af674-7584-447e-a65b-8294821511cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.src.metrics.base_metric.Mean at 0x7bbd02c33040>,\n",
              " <keras.src.metrics.accuracy_metrics.SparseCategoricalAccuracy at 0x7bbd02c33d90>]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 端到端实验示例1: 变分自动编码器"
      ],
      "metadata": {
        "id": "i8xfE-fw_3yF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class Sampling(layers.Layer):\n",
        "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var = inputs\n",
        "    batch = tf.shape(z_mean)[0]\n",
        "    dim = tf.shape(z_mean)[1]\n",
        "    # 创建一个均值为0，方差为1的正态分布张量\n",
        "    epsilon = keras.backend.random_normal(shape=(batch, dim))\n",
        "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "  \"\"\"Maps MNIST digits to a triplet (z_mean, z_los_var, z)\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim=32, intermediate_dim=64, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
        "    self.dense_mean = layers.Dense(latent_dim)\n",
        "    self.dense_log_var = layers.Dense(latent_dim)\n",
        "    self.sampling = Sampling()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_proj(inputs)\n",
        "    z_mean = self.dense_mean(x)\n",
        "    z_log_var = self.dense_log_var(x)\n",
        "    z = self.sampling((z_mean, z_log_var))\n",
        "    return z_mean, z_log_var, z"
      ],
      "metadata": {
        "id": "BCHKoosIyRn8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 接下来，我们有一个Decoder类，它将概率潜在空间坐标映射回MNIST数字\n",
        "class Decoder(layers.Layer):\n",
        "  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
        "\n",
        "  def __init__(self, original_dim, intermediate_dim=64, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.dense_proj = layers.Dense(intermediate_dim, activation=tf.nn.relu)\n",
        "    self.dense_output = layers.Dense(original_dim, activation=tf.nn.sigmoid)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.dense_proj(inputs)\n",
        "    return self.dense_output(x)"
      ],
      "metadata": {
        "id": "brW1IB52C8BG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 将编码器和解码器组合在一起\n",
        "class VariationalAutoEncoder(layers.Layer):\n",
        "  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
        "\n",
        "  def __init__(self, original_dim, intermediate_dim=64, latent_dim=32, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.original_dim = original_dim\n",
        "    self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
        "    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    z_mean, z_log_var, z = self.encoder(inputs)\n",
        "    reconstructed = self.decoder(z)\n",
        "\n",
        "    # Add KL divergence regularization loss\n",
        "    kl_loss = -0.5 * tf.reduce_mean(\n",
        "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1\n",
        "    )\n",
        "    self.add_loss(kl_loss)\n",
        "    return reconstructed"
      ],
      "metadata": {
        "id": "QNhME8ldDA5o"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 编写一个训练循环，我们的训练步骤用来@tf.function编译成超快速的图形函数\n",
        "# Our model\n",
        "vae = VariationalAutoEncoder(original_dim=784, intermediate_dim=64, latent_dim=32)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = keras.losses.MeanSquaredError()\n",
        "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "# Prepare a dataset\n",
        "(x_train, _), _ = keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    x_train.reshape(60000, 784).astype('float32') / 255\n",
        ")\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(32)\n",
        "\n",
        "@tf.function\n",
        "def training_step(x):\n",
        "  with tf.GradientTape() as tape:\n",
        "    reconstructed = vae(x)\n",
        "    # Compute loss\n",
        "    loss = loss_fn(x, reconstructed)\n",
        "    loss += sum(vae.losses)\n",
        "  # Update the weights of the VAE\n",
        "  grads = tape.gradient(loss, vae.trainable_weights)\n",
        "  optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "  return loss\n",
        "\n",
        "losses = []\n",
        "for step, x in enumerate(dataset):\n",
        "  loss = training_step(x)\n",
        "  # Logging\n",
        "  losses.append(float(loss))\n",
        "  if step % 100 == 0:\n",
        "    print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
        "\n",
        "  # Stop after 1000 steps\n",
        "  # Training the model to convergence is left\n",
        "  # as an exercise to the reader\n",
        "  if step >= 1000:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHstt0eOMbdW",
        "outputId": "63a6481b-3051-4a80-d1ef-65be71ee3cc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "Step: 0 Loss: 0.3274182379245758\n",
            "Step: 100 Loss: 0.12662266644805964\n",
            "Step: 200 Loss: 0.10012252639923523\n",
            "Step: 300 Loss: 0.08984106987021691\n",
            "Step: 400 Loss: 0.08494694289423878\n",
            "Step: 500 Loss: 0.08168212032157504\n",
            "Step: 600 Loss: 0.0792424521298952\n",
            "Step: 700 Loss: 0.07787219502467571\n",
            "Step: 800 Loss: 0.07664873064978144\n",
            "Step: 900 Loss: 0.07570356083051875\n",
            "Step: 1000 Loss: 0.07475208267048522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FzKhci-eSEzv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}